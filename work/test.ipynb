{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c88f7a-d9fd-4c32-a10d-3982d83f5b91",
   "metadata": {},
   "source": [
    "# Extract data to a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984230eb-ee17-4ac4-b737-202c09251c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Example: Reading a CSV file\n",
    "df = spark.read.option(\"sep\", \"\\t\").csv(\"data/en.openfoodfacts.org.products.csv.gz\", header=True, inferSchema=True).cache()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c02a34-5df3-41b9-bf0e-7dac5a401e0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generate a table in a CSV with statistical descriptions of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c509e0fd-0c95-448d-b884-9b9ab7bfb6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df.describe().toPandas().to_csv(\"describe_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5dbff-0230-4591-a711-21dc73918d26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Take a sample of the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d5f76d-d583-4802-9fa0-edc521eeb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRACTION_SIZE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d995a5-115b-42b1-a335-1ac1e7998960",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToSample = ['quantity', 'serving_size', 'serving_quantity', 'product_quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1ebc1d-f028-46f8-badf-46cc45681b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_not_clean_df = df.select(columnsToSample).dropna(how='all').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fe3f00-b57c-464f-a77c-b63e28e24461",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = samples_not_clean_df.sample(withReplacement=False, fraction=FRACTION_SIZE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ab1de3-9d35-4cf7-a291-fc21f5c03a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdbd707-1f7a-43a5-89bf-2d0e1c650129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------+----------------+\n",
      "|quantity|     serving_size|serving_quantity|product_quantity|\n",
      "+--------+-----------------+----------------+----------------+\n",
      "|   32 oz|             NULL|            NULL|       907.18474|\n",
      "|    NULL|     2 ONZ (57 g)|            57.0|            NULL|\n",
      "|   200 g|            100 g|           100.0|           200.0|\n",
      "|    NULL|  0.5 cup (125 g)|           125.0|            NULL|\n",
      "|    NULL|   0.5 cup (85 g)|            85.0|            NULL|\n",
      "|    NULL|38.3 g (1.35 ONZ)|            38.3|            NULL|\n",
      "|   6 lbs|             NULL|            NULL|          6000.0|\n",
      "|   540ml|             NULL|            NULL|           540.0|\n",
      "|   400 g|             NULL|            NULL|           400.0|\n",
      "|    NULL|            77.0g|            77.0|            NULL|\n",
      "|   400 g|             NULL|            NULL|           400.0|\n",
      "|    150g|             NULL|            NULL|           150.0|\n",
      "|    NULL|            100 g|           100.0|            NULL|\n",
      "|   100 g|             NULL|            NULL|           100.0|\n",
      "|   250ml|             NULL|            NULL|           250.0|\n",
      "+--------+-----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36aac7f-d961-4ad3-8a2e-b686e123db86",
   "metadata": {},
   "source": [
    "# Drop null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "654d9c42-65bb-45b0-8424-a6a4e46412f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_rows = df.dropna(subset=['categories_tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fbeb1-073d-4f49-8d3b-5dcb68323199",
   "metadata": {},
   "source": [
    "# Extract unique strings in a column (ingredients analysis tags in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac11a35-ffa3-4555-9475-21774e8e3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(li):\n",
    "    flat_list = []\n",
    "    for row in li:\n",
    "        flat_list += row\n",
    "    return flat_list\n",
    "\n",
    "def make_list_unique(li):\n",
    "    return list(dict.fromkeys(li))\n",
    "\n",
    "def split_string_list_elements(li, sep):\n",
    "    return [x.split(sep) for x in li]\n",
    "\n",
    "def column_to_list(col):\n",
    "    return col.rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f785cd80-da26-44c9-ad04-703fc19f1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = make_list_unique(flatten_list(split_string_list_elements(column_to_list(df.select('ingredients_analysis_tags').dropna()), \",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16092ba1-d5b4-44a1-884f-79649c1d1e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en:palm-oil-free',\n",
       " 'en:non-vegan',\n",
       " 'en:vegetarian-status-unknown',\n",
       " 'en:palm-oil-content-unknown',\n",
       " 'en:vegan-status-unknown',\n",
       " 'en:may-contain-palm-oil',\n",
       " 'en:vegetarian',\n",
       " 'en:vegan',\n",
       " 'en:non-vegetarian',\n",
       " 'en:palm-oil',\n",
       " 'en:maybe-vegan',\n",
       " 'en:maybe-vegetarian']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea218f-d625-42e0-9bc9-c9905418c902",
   "metadata": {},
   "source": [
    "# Drop unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdbddf06-6cc8-427d-b8ec-01b6724e754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_columns = [\"code\", \"product_quantity\", \"energy-kcal_100g\", \"fat_100g\", \"saturated-fat_100g\", \"unsaturated-fat_100g\", \"monounsaturated-fat_100g\", \"polyunsaturated-fat_100g\", \"trans-fat_100g\", \n",
    "                \"carbohydrates_100g\", \"sugars_100g\", \"added-sugars_100g\", \"starch_100g\", \"fiber_100g\", \"proteins_100g\", \"allergens\", \"traces\", \"vitamin-a_100g\", \"vitamin-c_100g\", \"vitamin-d_100g\",\n",
    "                \"vitamin-e_100g\", \"vitamin-k_100g\", \"vitamin-b1_100g\", \"vitamin-b2_100g\", \"vitamin-b6_100g\", \"vitamin-b9_100g\", \"vitamin-b12_100g\", \"calcium_100g\",\n",
    "                \"iron_100g\", \"magnesium_100g\", \"potassium_100g\", \"zinc_100g\", \"food_groups_tags\", \"serving_size\", \"serving_quantity\", \"cholesterol_100g\", \"salt_100g\", \"glycemic-index_100g\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eee5f531-00a6-4b91-b5a2-b96c156ac3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[code: double, url: string, creator: string, created_t: int, created_datetime: timestamp, last_modified_t: int, last_modified_datetime: timestamp, last_modified_by: string, last_updated_t: int, last_updated_datetime: timestamp, product_name: string, abbreviated_product_name: string, generic_name: string, quantity: string, packaging: string, packaging_tags: string, packaging_en: string, packaging_text: string, brands: string, brands_tags: string, categories: string, categories_tags: string, categories_en: string, origins: string, origins_tags: string, origins_en: string, manufacturing_places: string, manufacturing_places_tags: string, labels: string, labels_tags: string, labels_en: string, emb_codes: string, emb_codes_tags: string, first_packaging_code_geo: string, cities: string, cities_tags: string, purchase_places: string, stores: string, countries: string, countries_tags: string, countries_en: string, ingredients_text: string, ingredients_tags: string, ingredients_analysis_tags: string, allergens: string, allergens_en: string, traces: string, traces_tags: string, traces_en: string, serving_size: string, serving_quantity: double, no_nutrition_data: string, additives_n: int, additives: string, additives_tags: string, additives_en: string, nutriscore_score: int, nutriscore_grade: string, nova_group: int, pnns_groups_1: string, pnns_groups_2: string, food_groups: string, food_groups_tags: string, food_groups_en: string, states: string, states_tags: string, states_en: string, brand_owner: string, ecoscore_score: double, ecoscore_grade: string, nutrient_levels_tags: string, product_quantity: double, owner: string, data_quality_errors_tags: string, unique_scans_n: int, popularity_tags: string, completeness: double, last_image_t: int, last_image_datetime: timestamp, main_category: string, main_category_en: string, image_url: string, image_small_url: string, image_ingredients_url: string, image_ingredients_small_url: string, image_nutrition_url: string, image_nutrition_small_url: string, energy-kj_100g: double, energy-kcal_100g: double, energy_100g: double, energy-from-fat_100g: double, fat_100g: double, saturated-fat_100g: double, butyric-acid_100g: double, caproic-acid_100g: double, caprylic-acid_100g: double, capric-acid_100g: double, lauric-acid_100g: double, myristic-acid_100g: double, palmitic-acid_100g: double, stearic-acid_100g: double, arachidic-acid_100g: double, behenic-acid_100g: double, lignoceric-acid_100g: double, cerotic-acid_100g: double, montanic-acid_100g: double, melissic-acid_100g: double, unsaturated-fat_100g: double, monounsaturated-fat_100g: double, omega-9-fat_100g: double, polyunsaturated-fat_100g: double, omega-3-fat_100g: double, omega-6-fat_100g: double, alpha-linolenic-acid_100g: double, eicosapentaenoic-acid_100g: double, docosahexaenoic-acid_100g: double, linoleic-acid_100g: double, arachidonic-acid_100g: double, gamma-linolenic-acid_100g: double, dihomo-gamma-linolenic-acid_100g: double, oleic-acid_100g: double, elaidic-acid_100g: double, gondoic-acid_100g: double, mead-acid_100g: double, erucic-acid_100g: double, nervonic-acid_100g: double, trans-fat_100g: double, cholesterol_100g: double, carbohydrates_100g: double, sugars_100g: double, added-sugars_100g: double, sucrose_100g: double, glucose_100g: double, fructose_100g: double, lactose_100g: double, maltose_100g: double, maltodextrins_100g: double, starch_100g: double, polyols_100g: double, erythritol_100g: double, fiber_100g: double, soluble-fiber_100g: double, insoluble-fiber_100g: double, proteins_100g: double, casein_100g: double, serum-proteins_100g: double, nucleotides_100g: double, salt_100g: double, added-salt_100g: double, sodium_100g: double, alcohol_100g: double, vitamin-a_100g: double, beta-carotene_100g: double, vitamin-d_100g: double, vitamin-e_100g: double, vitamin-k_100g: double, vitamin-c_100g: double, vitamin-b1_100g: double, vitamin-b2_100g: double, vitamin-pp_100g: double, vitamin-b6_100g: double, vitamin-b9_100g: double, folates_100g: double, vitamin-b12_100g: double, biotin_100g: double, pantothenic-acid_100g: double, silica_100g: double, bicarbonate_100g: double, potassium_100g: double, chloride_100g: double, calcium_100g: double, phosphorus_100g: double, iron_100g: double, magnesium_100g: double, zinc_100g: double, copper_100g: double, manganese_100g: double, fluoride_100g: double, selenium_100g: double, chromium_100g: double, molybdenum_100g: double, iodine_100g: double, caffeine_100g: double, taurine_100g: double, ph_100g: double, fruits-vegetables-nuts_100g: double, fruits-vegetables-nuts-dried_100g: double, fruits-vegetables-nuts-estimate_100g: double, fruits-vegetables-nuts-estimate-from-ingredients_100g: double, collagen-meat-protein-ratio_100g: double, cocoa_100g: double, chlorophyl_100g: double, carbon-footprint_100g: double, carbon-footprint-from-meat-or-fish_100g: double, nutrition-score-fr_100g: int, nutrition-score-uk_100g: int, glycemic-index_100g: double, water-hardness_100g: double, choline_100g: double, phylloquinone_100g: double, beta-glucan_100g: double, inositol_100g: double, carnitine_100g: double, sulphate_100g: double, nitrate_100g: double, acidity_100g: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kept_columns = df.select(kept_columns).cache()\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fa1d0-2f4a-4fc6-94a2-6b10cbf605ad",
   "metadata": {},
   "source": [
    "# Write DF to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1c37c-164a-4d0e-88e2-56080b24284e",
   "metadata": {},
   "source": [
    "## Get user's database credentials (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80dbeb9e-fdd3-4d15-9844-a2a37c0e1047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties = {}\n",
    "# url = {}\n",
    "# for text in [\"database URL\", \"table name\"]\n",
    "#     url[text] = input(\"Enter \" + text + \": \")\n",
    "\n",
    "\n",
    "# for text in [\"user\", \"password\", \"driver\"]:\n",
    "#     properties[text] = input(\"Enter \" + text + \": \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ee6e3-219e-4c14-adfc-5c96b038b20d",
   "metadata": {},
   "source": [
    "## Write to the database using JDBC Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2722c1f-7bb7-4414-bbc6-cb26c63d2896",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o137.jdbc.\n: java.sql.SQLSyntaxErrorException: Access denied for user 'user'@'%' to database 'pyspark_db'\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:828)\n\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:448)\n\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:241)\n\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.mysql.cj.jdbc.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf_kept_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:mysql://mysql:3306/pyspark_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproducts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o137.jdbc.\n: java.sql.SQLSyntaxErrorException: Access denied for user 'user'@'%' to database 'pyspark_db'\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:828)\n\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:448)\n\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:241)\n\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "properties = {\n",
    "    \"user\": \"user\",\n",
    "    \"password\": \"userpassword\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "df_kept_columns.write.jdbc(url=\"jdbc:mysql://mysql:3306/pyspark_db\", table=\"products\", mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed01e5f-e4e7-46fc-af7a-1b6c314e25c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GPT help"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6e10934-67c4-4c31-8fd5-2a340d7b00db",
   "metadata": {},
   "source": [
    "To create a vegetarian menu based on the data you have, you can leverage certain columns in your dataset that contain relevant information about the product's ingredients and nutritional analysis. Specifically, you can use the following steps:\n",
    "\n",
    "Identify Relevant Columns: The columns that might help determine whether a product is vegetarian-friendly could include:\n",
    "\n",
    "ingredients_text: Describes the ingredients in the product.\n",
    "ingredients_analysis_tags: May contain tags related to the type of ingredients (e.g., \"vegetarian\", \"vegan\", \"meat\").\n",
    "food_groups, food_groups_tags, categories, categories_tags: These columns may classify the product into food categories or groups that indicate if the product is vegetarian-friendly.\n",
    "additives_tags: Might include information about additives that are animal-derived.\n",
    "ecoscore_grade, nutriscore_grade, nutriscore_score: Could provide insights into the sustainability or nutritional value of the product.\n",
    "Filter for Vegetarian Tags or Keywords:\n",
    "\n",
    "You can filter products where the ingredients_analysis_tags or categories_tags include tags such as \"vegetarian\" or exclude those with tags like \"meat\", \"animal\", or specific animal products such as \"beef\", \"chicken\", \"pork\", \"fish\", \"gelatin\", etc.\n",
    "Create a Filtered Vegetarian Menu: You can use PySpark to filter and create a new DataFrame for vegetarian products. Below is an example code that demonstrates how you could do this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "175383a5-94ad-4878-809a-2dc1c7d03080",
   "metadata": {},
   "source": [
    "1. Calories and Macronutrients\n",
    "    These are essential for understanding the caloric content and macronutrient distribution, which is crucial for diets like keto, low-carb, high-protein, and balanced diets:\n",
    "    \n",
    "    energy-kcal_100g\n",
    "    fat_100g\n",
    "    saturated-fat_100g\n",
    "    unsaturated-fat_100g\n",
    "    monounsaturated-fat_100g\n",
    "    polyunsaturated-fat_100g\n",
    "    trans-fat_100g\n",
    "    carbohydrates_100g\n",
    "    sugars_100g\n",
    "    added-sugars_100g\n",
    "    starch_100g\n",
    "    fiber_100g\n",
    "    proteins_100g\n",
    "\n",
    "2. Allergens and Traces\n",
    "    For allergen-free diets (e.g., gluten-free, dairy-free, nut-free, etc.):\n",
    "    \n",
    "    allergens\n",
    "    allergens_en\n",
    "    traces\n",
    "    traces_en\n",
    "\n",
    "3. Additives and Preservatives\n",
    "    Useful for clean eating, whole-food, or organic diets that avoid artificial ingredients:\n",
    "    \n",
    "    additives_n\n",
    "    additives\n",
    "    additives_tags\n",
    "    additives_en\n",
    "\n",
    "4. Vitamins and Minerals\n",
    "    Important for nutrient-rich diets (e.g., vegan, vegetarian, or those focusing on micronutrient density):\n",
    "    \n",
    "    vitamin-a_100g\n",
    "    vitamin-c_100g\n",
    "    vitamin-d_100g\n",
    "    vitamin-e_100g\n",
    "    vitamin-k_100g\n",
    "    vitamin-b1_100g\n",
    "    vitamin-b2_100g\n",
    "    vitamin-b6_100g\n",
    "    vitamin-b9_100g\n",
    "    vitamin-b12_100g\n",
    "    calcium_100g\n",
    "    iron_100g\n",
    "    magnesium_100g\n",
    "    potassium_100g\n",
    "    zinc_100g\n",
    "\n",
    "5. Food Origin and Labels\n",
    "    Helpful for organic, fair trade, local sourcing, or sustainability-focused diets:\n",
    "    \n",
    "    origins\n",
    "    origins_tags\n",
    "    labels\n",
    "    labels_tags\n",
    "    labels_en\n",
    "    ecoscore_score\n",
    "    ecoscore_grade\n",
    "    carbon-footprint_100g\n",
    "    carbon-footprint-from-meat-or-fish_100g\n",
    "\n",
    "6. Product Composition\n",
    "    For vegetarian, vegan, or plant-based diets:\n",
    "    \n",
    "    ingredients_text\n",
    "    ingredients_tags\n",
    "    fruits-vegetables-nuts_100g\n",
    "    fruits-vegetables-nuts-dried_100g\n",
    "    fruits-vegetables-nuts-estimate_100g\n",
    "    fruits-vegetables-nuts-estimate-from-ingredients_100g\n",
    "\n",
    "7. Serving Size and Quantities\n",
    "    For managing portion control and specific caloric intake:\n",
    "    \n",
    "    serving_size\n",
    "    serving_quantity\n",
    "\n",
    "8. Nutritional Quality and Scores\n",
    "    Useful for general health-conscious eating:\n",
    "    \n",
    "    nutriscore_score\n",
    "    nutriscore_grade\n",
    "    nutrition-score-fr_100g\n",
    "    nutrition-score-uk_100g\n",
    "\n",
    "9. Specific Components\n",
    "    Helpful for specialized diets focusing on specific nutrients (e.g., low-sodium, low-sugar, low-cholesterol diets):\n",
    "    \n",
    "    cholesterol_100g\n",
    "    sodium_100g\n",
    "    salt_100g\n",
    "    added-salt_100g\n",
    "    glycemic-index_100g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
